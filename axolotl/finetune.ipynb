{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aki/workspace/learning/finetuning/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 100%|██████████| 52002/52002 [00:00<00:00, 906964.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"tatsu-lab/alpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text'],\n",
       "    num_rows: 52002\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-23 16:22:07,018] [INFO] [numexpr.utils._init_num_threads:149] [PID:2266767] Note: NumExpr detected 24 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-01-23 16:22:07,018] [INFO] [numexpr.utils._init_num_threads:162] [PID:2266767] NumExpr defaulting to 16 threads.\n",
      "[2025-01-23 16:22:07,077] [INFO] [datasets.<module>:54] [PID:2266767] PyTorch version 2.5.1 available.\n",
      "[2025-01-23 16:22:07,596] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-01-23 16:22:07,623] [INFO] [root.spawn:60] [PID:2266767] gcc -pthread -B /home/aki/miniconda3/compiler_compat -fno-strict-overflow -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/aki/miniconda3/include -fPIC -O2 -isystem /home/aki/miniconda3/include -fPIC -c /tmp/tmpkold70wi/test.c -o /tmp/tmpkold70wi/test.o\n",
      "[2025-01-23 16:22:07,632] [INFO] [root.spawn:60] [PID:2266767] gcc -pthread -B /home/aki/miniconda3/compiler_compat /tmp/tmpkold70wi/test.o -laio -o /tmp/tmpkold70wi/a.out\n",
      "[2025-01-23 16:22:07,819] [INFO] [root.spawn:60] [PID:2266767] gcc -pthread -B /home/aki/miniconda3/compiler_compat -fno-strict-overflow -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/aki/miniconda3/include -fPIC -O2 -isystem /home/aki/miniconda3/include -fPIC -c /tmp/tmp12etyg1d/test.c -o /tmp/tmp12etyg1d/test.o\n",
      "[2025-01-23 16:22:07,829] [INFO] [root.spawn:60] [PID:2266767] gcc -pthread -B /home/aki/miniconda3/compiler_compat /tmp/tmp12etyg1d/test.o -L/usr/local/cuda -L/usr/local/cuda/lib64 -lcufile -o /tmp/tmp12etyg1d/a.out\n",
      "/home/aki/workspace/learning/finetuning/.venv/lib/python3.12/site-packages/axolotl/monkeypatch/relora.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n",
      "\n",
      "     #@@ #@@      @@# @@#\n",
      "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
      "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
      "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
      "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
      "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
      "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
      "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
      "    @@@@  @@@@@@@@@@@@@@@@\n",
      "\n",
      "[2025-01-23 16:22:08,401] [INFO] [axolotl.normalize_config:211] [PID:2266767] [RANK:0] cuda memory usage baseline: 0.000GB (+1.762GB misc)\u001b[39m\n",
      "[2025-01-23 16:22:09,578] [DEBUG] [axolotl.load_tokenizer:296] [PID:2266767] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
      "[2025-01-23 16:22:09,578] [DEBUG] [axolotl.load_tokenizer:297] [PID:2266767] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
      "[2025-01-23 16:22:09,578] [DEBUG] [axolotl.load_tokenizer:298] [PID:2266767] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
      "[2025-01-23 16:22:09,578] [DEBUG] [axolotl.load_tokenizer:299] [PID:2266767] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
      "[2025-01-23 16:22:09,579] [INFO] [axolotl.load_tokenized_prepared_datasets:212] [PID:2266767] [RANK:0] Skipping prepared dataset in last_run_prepared/4ffa9e604bbfc10574f229b43f9234a2 for pre-processing...\u001b[39m\n",
      "[2025-01-23 16:22:09,579] [INFO] [axolotl.load_tokenized_prepared_datasets:217] [PID:2266767] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "Generating train split: 100 examples [00:00, 58383.96 examples/s]\n",
      "[2025-01-23 16:22:10,180] [INFO] [axolotl.get_dataset_wrapper:613] [PID:2266767] [RANK:0] Loading dataset with base_type: alpaca and prompt_style: None\u001b[39m\n",
      "Tokenizing Prompts (num_proc=24): 100%|█| 100/100 [00:00<00:00, 586.71 examples/\n",
      "[2025-01-23 16:22:10,520] [INFO] [axolotl.load_tokenized_prepared_datasets:486] [PID:2266767] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/4ffa9e604bbfc10574f229b43f9234a2\u001b[39m\n",
      "Saving the dataset (1/1 shards): 100%|█| 100/100 [00:00<00:00, 28439.82 examples\n",
      "[2025-01-23 16:22:10,527] [DEBUG] [axolotl.calculate_total_num_steps:342] [PID:2266767] [RANK:0] total_num_tokens: 47_741\u001b[39m\n",
      "[2025-01-23 16:22:10,528] [DEBUG] [axolotl.calculate_total_num_steps:360] [PID:2266767] [RANK:0] `total_supervised_tokens: 6_994`\u001b[39m\n",
      "[2025-01-23 16:22:10,528] [DEBUG] [axolotl.calculate_total_num_steps:438] [PID:2266767] [RANK:0] total_num_steps: 6\u001b[39m\n",
      "[2025-01-23 16:22:10,528] [INFO] [axolotl.scripts.load_datasets:486] [PID:2266767] [RANK:0] check_dataset_labels...\u001b[39m\n",
      "[2025-01-23 16:22:10,535] [INFO] [axolotl.check_example_labels:43] [PID:2266767] [RANK:0] \u001b[31m<s>\u001b[0m\u001b[97m(-100, 1)\u001b[0m \u001b[31mBel\u001b[0m\u001b[97m(-100, 20548)\u001b[0m \u001b[31mow\u001b[0m\u001b[97m(-100, 336)\u001b[0m \u001b[31m is\u001b[0m\u001b[97m(-100, 349)\u001b[0m \u001b[31m an\u001b[0m\u001b[97m(-100, 396)\u001b[0m \u001b[31m instruction\u001b[0m\u001b[97m(-100, 13126)\u001b[0m \u001b[31m that\u001b[0m\u001b[97m(-100, 369)\u001b[0m \u001b[31m describes\u001b[0m\u001b[97m(-100, 13966)\u001b[0m \u001b[31m a\u001b[0m\u001b[97m(-100, 264)\u001b[0m \u001b[31m task\u001b[0m\u001b[97m(-100, 3638)\u001b[0m \u001b[31m,\u001b[0m\u001b[97m(-100, 28725)\u001b[0m \u001b[31m pa\u001b[0m\u001b[97m(-100, 5881)\u001b[0m \u001b[31mired\u001b[0m\u001b[97m(-100, 1360)\u001b[0m \u001b[31m with\u001b[0m\u001b[97m(-100, 395)\u001b[0m \u001b[31m an\u001b[0m\u001b[97m(-100, 396)\u001b[0m \u001b[31m input\u001b[0m\u001b[97m(-100, 2787)\u001b[0m \u001b[31m that\u001b[0m\u001b[97m(-100, 369)\u001b[0m \u001b[31m provides\u001b[0m\u001b[97m(-100, 5312)\u001b[0m \u001b[31m further\u001b[0m\u001b[97m(-100, 3629)\u001b[0m \u001b[31m context\u001b[0m\u001b[97m(-100, 2758)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31m Write\u001b[0m\u001b[97m(-100, 12018)\u001b[0m \u001b[31m a\u001b[0m\u001b[97m(-100, 264)\u001b[0m \u001b[31m response\u001b[0m\u001b[97m(-100, 2899)\u001b[0m \u001b[31m that\u001b[0m\u001b[97m(-100, 369)\u001b[0m \u001b[31m appropri\u001b[0m\u001b[97m(-100, 6582)\u001b[0m \u001b[31mately\u001b[0m\u001b[97m(-100, 1999)\u001b[0m \u001b[31m complet\u001b[0m\u001b[97m(-100, 2691)\u001b[0m \u001b[31mes\u001b[0m\u001b[97m(-100, 274)\u001b[0m \u001b[31m the\u001b[0m\u001b[97m(-100, 272)\u001b[0m \u001b[31m request\u001b[0m\u001b[97m(-100, 2159)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31m\n",
      "\u001b[0m\u001b[97m(-100, 13)\u001b[0m \u001b[31m\n",
      "\u001b[0m\u001b[97m(-100, 13)\u001b[0m \u001b[31m###\u001b[0m\u001b[97m(-100, 27332)\u001b[0m \u001b[31m Inst\u001b[0m\u001b[97m(-100, 3133)\u001b[0m \u001b[31mruction\u001b[0m\u001b[97m(-100, 3112)\u001b[0m \u001b[31m:\u001b[0m\u001b[97m(-100, 28747)\u001b[0m \u001b[31m\n",
      "\u001b[0m\u001b[97m(-100, 13)\u001b[0m \u001b[31mH\u001b[0m\u001b[97m(-100, 28769)\u001b[0m \u001b[31money\u001b[0m\u001b[97m(-100, 2011)\u001b[0m \u001b[31mcomb\u001b[0m\u001b[97m(-100, 17305)\u001b[0m \u001b[31m is\u001b[0m\u001b[97m(-100, 349)\u001b[0m \u001b[31m an\u001b[0m\u001b[97m(-100, 396)\u001b[0m \u001b[31m observ\u001b[0m\u001b[97m(-100, 3977)\u001b[0m \u001b[31mability\u001b[0m\u001b[97m(-100, 2437)\u001b[0m \u001b[31m platform\u001b[0m\u001b[97m(-100, 5181)\u001b[0m \u001b[31m that\u001b[0m\u001b[97m(-100, 369)\u001b[0m \u001b[31m allows\u001b[0m\u001b[97m(-100, 5976)\u001b[0m \u001b[31m you\u001b[0m\u001b[97m(-100, 368)\u001b[0m \u001b[31m to\u001b[0m\u001b[97m(-100, 298)\u001b[0m \u001b[31m write\u001b[0m\u001b[97m(-100, 3324)\u001b[0m \u001b[31m queries\u001b[0m\u001b[97m(-100, 23681)\u001b[0m \u001b[31m to\u001b[0m\u001b[97m(-100, 298)\u001b[0m \u001b[31m inspect\u001b[0m\u001b[97m(-100, 16279)\u001b[0m \u001b[31m trace\u001b[0m\u001b[97m(-100, 10718)\u001b[0m \u001b[31m data\u001b[0m\u001b[97m(-100, 1178)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31m You\u001b[0m\u001b[97m(-100, 995)\u001b[0m \u001b[31m are\u001b[0m\u001b[97m(-100, 460)\u001b[0m \u001b[31m an\u001b[0m\u001b[97m(-100, 396)\u001b[0m \u001b[31m assistant\u001b[0m\u001b[97m(-100, 13892)\u001b[0m \u001b[31m that\u001b[0m\u001b[97m(-100, 369)\u001b[0m \u001b[31m takes\u001b[0m\u001b[97m(-100, 4347)\u001b[0m \u001b[31m a\u001b[0m\u001b[97m(-100, 264)\u001b[0m \u001b[31m natural\u001b[0m\u001b[97m(-100, 4229)\u001b[0m \u001b[31m language\u001b[0m\u001b[97m(-100, 3842)\u001b[0m \u001b[31m query\u001b[0m\u001b[97m(-100, 5709)\u001b[0m \u001b[31m (\u001b[0m\u001b[97m(-100, 325)\u001b[0m \u001b[31mNL\u001b[0m\u001b[97m(-100, 16717)\u001b[0m \u001b[31mQ\u001b[0m\u001b[97m(-100, 28824)\u001b[0m \u001b[31m)\u001b[0m\u001b[97m(-100, 28731)\u001b[0m \u001b[31m and\u001b[0m\u001b[97m(-100, 304)\u001b[0m \u001b[31m a\u001b[0m\u001b[97m(-100, 264)\u001b[0m \u001b[31m list\u001b[0m\u001b[97m(-100, 1274)\u001b[0m \u001b[31m of\u001b[0m\u001b[97m(-100, 302)\u001b[0m \u001b[31m valid\u001b[0m\u001b[97m(-100, 3716)\u001b[0m \u001b[31m columns\u001b[0m\u001b[97m(-100, 11252)\u001b[0m \u001b[31m and\u001b[0m\u001b[97m(-100, 304)\u001b[0m \u001b[31m produce\u001b[0m\u001b[97m(-100, 7072)\u001b[0m \u001b[31m a\u001b[0m\u001b[97m(-100, 264)\u001b[0m \u001b[31m H\u001b[0m\u001b[97m(-100, 382)\u001b[0m \u001b[31money\u001b[0m\u001b[97m(-100, 2011)\u001b[0m \u001b[31mcomb\u001b[0m\u001b[97m(-100, 17305)\u001b[0m \u001b[31m query\u001b[0m\u001b[97m(-100, 5709)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31m\n",
      "\u001b[0m\u001b[97m(-100, 13)\u001b[0m \u001b[31m\n",
      "\u001b[0m\u001b[97m(-100, 13)\u001b[0m \u001b[31m###\u001b[0m\u001b[97m(-100, 27332)\u001b[0m \u001b[31m Input\u001b[0m\u001b[97m(-100, 11232)\u001b[0m \u001b[31m:\u001b[0m\u001b[97m(-100, 28747)\u001b[0m \u001b[31m\n",
      "\u001b[0m\u001b[97m(-100, 13)\u001b[0m \u001b[31m\n",
      "\u001b[0m\u001b[97m(-100, 13)\u001b[0m \u001b[31m\n",
      "\u001b[0m\u001b[97m(-100, 13)\u001b[0m \u001b[31mNL\u001b[0m\u001b[97m(-100, 16717)\u001b[0m \u001b[31mQ\u001b[0m\u001b[97m(-100, 28824)\u001b[0m \u001b[31m:\u001b[0m\u001b[97m(-100, 28747)\u001b[0m \u001b[31m \"\u001b[0m\u001b[97m(-100, 345)\u001b[0m \u001b[31mTop\u001b[0m\u001b[97m(-100, 6228)\u001b[0m \u001b[31m long\u001b[0m\u001b[97m(-100, 1043)\u001b[0m \u001b[31m-\u001b[0m\u001b[97m(-100, 28733)\u001b[0m \u001b[31mrunning\u001b[0m\u001b[97m(-100, 17316)\u001b[0m \u001b[31m database\u001b[0m\u001b[97m(-100, 7499)\u001b[0m \u001b[31m operations\u001b[0m\u001b[97m(-100, 6933)\u001b[0m \u001b[31m in\u001b[0m\u001b[97m(-100, 297)\u001b[0m \u001b[31m the\u001b[0m\u001b[97m(-100, 272)\u001b[0m \u001b[31m bottom\u001b[0m\u001b[97m(-100, 5859)\u001b[0m \u001b[31m \u001b[0m\u001b[97m(-100, 28705)\u001b[0m \u001b[31m1\u001b[0m\u001b[97m(-100, 28740)\u001b[0m \u001b[31m0\u001b[0m\u001b[97m(-100, 28734)\u001b[0m \u001b[31m%\u001b[0m\u001b[97m(-100, 28823)\u001b[0m \u001b[31m percent\u001b[0m\u001b[97m(-100, 4153)\u001b[0m \u001b[31mile\u001b[0m\u001b[97m(-100, 546)\u001b[0m \u001b[31m\"\u001b[0m\u001b[97m(-100, 28739)\u001b[0m \u001b[31m\n",
      "\u001b[0m\u001b[97m(-100, 13)\u001b[0m \u001b[31m\n",
      "\u001b[0m\u001b[97m(-100, 13)\u001b[0m \u001b[31mColumns\u001b[0m\u001b[97m(-100, 15468)\u001b[0m \u001b[31m:\u001b[0m\u001b[97m(-100, 28747)\u001b[0m \u001b[31m ['\u001b[0m\u001b[97m(-100, 5936)\u001b[0m \u001b[31mparent\u001b[0m\u001b[97m(-100, 3682)\u001b[0m \u001b[31m_\u001b[0m\u001b[97m(-100, 28730)\u001b[0m \u001b[31mname\u001b[0m\u001b[97m(-100, 861)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mduration\u001b[0m\u001b[97m(-100, 15458)\u001b[0m \u001b[31m_\u001b[0m\u001b[97m(-100, 28730)\u001b[0m \u001b[31mms\u001b[0m\u001b[97m(-100, 1033)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mdb\u001b[0m\u001b[97m(-100, 2684)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31msystem\u001b[0m\u001b[97m(-100, 6574)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mlogin\u001b[0m\u001b[97m(-100, 10118)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mleg\u001b[0m\u001b[97m(-100, 1946)\u001b[0m \u001b[31macy\u001b[0m\u001b[97m(-100, 2426)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mrpc\u001b[0m\u001b[97m(-100, 19390)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31msystem\u001b[0m\u001b[97m(-100, 6574)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mdb\u001b[0m\u001b[97m(-100, 2684)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31muser\u001b[0m\u001b[97m(-100, 1838)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31merror\u001b[0m\u001b[97m(-100, 1958)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mnet\u001b[0m\u001b[97m(-100, 1687)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mtransport\u001b[0m\u001b[97m(-100, 19645)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mnet\u001b[0m\u001b[97m(-100, 1687)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mhost\u001b[0m\u001b[97m(-100, 3404)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mname\u001b[0m\u001b[97m(-100, 861)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mname\u001b[0m\u001b[97m(-100, 861)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mtrace\u001b[0m\u001b[97m(-100, 9213)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mparent\u001b[0m\u001b[97m(-100, 3682)\u001b[0m \u001b[31m_\u001b[0m\u001b[97m(-100, 28730)\u001b[0m \u001b[31mid\u001b[0m\u001b[97m(-100, 313)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mhttp\u001b[0m\u001b[97m(-100, 2872)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mtarget\u001b[0m\u001b[97m(-100, 3731)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mrpc\u001b[0m\u001b[97m(-100, 19390)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mservice\u001b[0m\u001b[97m(-100, 5134)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mmeta\u001b[0m\u001b[97m(-100, 8301)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mref\u001b[0m\u001b[97m(-100, 1013)\u001b[0m \u001b[31minery\u001b[0m\u001b[97m(-100, 17171)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mreason\u001b[0m\u001b[97m(-100, 14991)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mrpc\u001b[0m\u001b[97m(-100, 19390)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mmethod\u001b[0m\u001b[97m(-100, 4350)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mspan\u001b[0m\u001b[97m(-100, 3721)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mkind\u001b[0m\u001b[97m(-100, 9186)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mdb\u001b[0m\u001b[97m(-100, 2684)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mstatement\u001b[0m\u001b[97m(-100, 22910)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mreport\u001b[0m\u001b[97m(-100, 10146)\u001b[0m \u001b[31m_\u001b[0m\u001b[97m(-100, 28730)\u001b[0m \u001b[31mdepend\u001b[0m\u001b[97m(-100, 11569)\u001b[0m \u001b[31mencies\u001b[0m\u001b[97m(-100, 6094)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mnum\u001b[0m\u001b[97m(-100, 2575)\u001b[0m \u001b[31m_\u001b[0m\u001b[97m(-100, 28730)\u001b[0m \u001b[31mph\u001b[0m\u001b[97m(-100, 721)\u001b[0m \u001b[31motos\u001b[0m\u001b[97m(-100, 26064)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mhttp\u001b[0m\u001b[97m(-100, 2872)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31murl\u001b[0m\u001b[97m(-100, 2179)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mmeta\u001b[0m\u001b[97m(-100, 8301)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mref\u001b[0m\u001b[97m(-100, 1013)\u001b[0m \u001b[31minery\u001b[0m\u001b[97m(-100, 17171)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31moriginal\u001b[0m\u001b[97m(-100, 11802)\u001b[0m \u001b[31m_\u001b[0m\u001b[97m(-100, 28730)\u001b[0m \u001b[31msample\u001b[0m\u001b[97m(-100, 11493)\u001b[0m \u001b[31m_\u001b[0m\u001b[97m(-100, 28730)\u001b[0m \u001b[31mrate\u001b[0m\u001b[97m(-100, 6036)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mlogin\u001b[0m\u001b[97m(-100, 10118)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mprovider\u001b[0m\u001b[97m(-100, 14620)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mlogin\u001b[0m\u001b[97m(-100, 10118)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31ms\u001b[0m\u001b[97m(-100, 28713)\u001b[0m \u001b[31mso\u001b[0m\u001b[97m(-100, 667)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mtype\u001b[0m\u001b[97m(-100, 1123)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mreport\u001b[0m\u001b[97m(-100, 10146)\u001b[0m \u001b[31m_\u001b[0m\u001b[97m(-100, 28730)\u001b[0m \u001b[31mid\u001b[0m\u001b[97m(-100, 313)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mlibrary\u001b[0m\u001b[97m(-100, 16228)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mname\u001b[0m\u001b[97m(-100, 861)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mf\u001b[0m\u001b[97m(-100, 28722)\u001b[0m \u001b[31musion\u001b[0m\u001b[97m(-100, 3623)\u001b[0m \u001b[31mauth\u001b[0m\u001b[97m(-100, 6414)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mfind\u001b[0m\u001b[97m(-100, 3326)\u001b[0m \u001b[31m_\u001b[0m\u001b[97m(-100, 28730)\u001b[0m \u001b[31muser\u001b[0m\u001b[97m(-100, 1838)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mresponse\u001b[0m\u001b[97m(-100, 4439)\u001b[0m \u001b[31m_\u001b[0m\u001b[97m(-100, 28730)\u001b[0m \u001b[31mtotal\u001b[0m\u001b[97m(-100, 6828)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mmeta\u001b[0m\u001b[97m(-100, 8301)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mannotation\u001b[0m\u001b[97m(-100, 10066)\u001b[0m \u001b[31m_\u001b[0m\u001b[97m(-100, 28730)\u001b[0m \u001b[31mtype\u001b[0m\u001b[97m(-100, 1123)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mservice\u001b[0m\u001b[97m(-100, 5134)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mname\u001b[0m\u001b[97m(-100, 861)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mreport\u001b[0m\u001b[97m(-100, 10146)\u001b[0m \u001b[31m_\u001b[0m\u001b[97m(-100, 28730)\u001b[0m \u001b[31mdepend\u001b[0m\u001b[97m(-100, 11569)\u001b[0m \u001b[31mencies\u001b[0m\u001b[97m(-100, 6094)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mnum\u001b[0m\u001b[97m(-100, 2575)\u001b[0m \u001b[31m_\u001b[0m\u001b[97m(-100, 28730)\u001b[0m \u001b[31msign\u001b[0m\u001b[97m(-100, 6301)\u001b[0m \u001b[31mator\u001b[0m\u001b[97m(-100, 1028)\u001b[0m \u001b[31mies\u001b[0m\u001b[97m(-100, 497)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mnet\u001b[0m\u001b[97m(-100, 1687)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mhost\u001b[0m\u001b[97m(-100, 3404)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mport\u001b[0m\u001b[97m(-100, 483)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mmessage\u001b[0m\u001b[97m(-100, 1324)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mtype\u001b[0m\u001b[97m(-100, 1123)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mlibrary\u001b[0m\u001b[97m(-100, 16228)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mversion\u001b[0m\u001b[97m(-100, 1790)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mexception\u001b[0m\u001b[97m(-100, 7703)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mtype\u001b[0m\u001b[97m(-100, 1123)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mtrace\u001b[0m\u001b[97m(-100, 9213)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mtrace\u001b[0m\u001b[97m(-100, 9213)\u001b[0m \u001b[31m_\u001b[0m\u001b[97m(-100, 28730)\u001b[0m \u001b[31mid\u001b[0m\u001b[97m(-100, 313)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mins\u001b[0m\u001b[97m(-100, 1126)\u001b[0m \u001b[31mpection\u001b[0m\u001b[97m(-100, 16455)\u001b[0m \u001b[31m_\u001b[0m\u001b[97m(-100, 28730)\u001b[0m \u001b[31mid\u001b[0m\u001b[97m(-100, 313)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mspan\u001b[0m\u001b[97m(-100, 3721)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mnum\u001b[0m\u001b[97m(-100, 2575)\u001b[0m \u001b[31m_\u001b[0m\u001b[97m(-100, 28730)\u001b[0m \u001b[31mlinks\u001b[0m\u001b[97m(-100, 17052)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mf\u001b[0m\u001b[97m(-100, 28722)\u001b[0m \u001b[31musion\u001b[0m\u001b[97m(-100, 3623)\u001b[0m \u001b[31mauth\u001b[0m\u001b[97m(-100, 6414)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mfind\u001b[0m\u001b[97m(-100, 3326)\u001b[0m \u001b[31m_\u001b[0m\u001b[97m(-100, 28730)\u001b[0m \u001b[31muser\u001b[0m\u001b[97m(-100, 1838)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 28723)\u001b[0m \u001b[31mrecord\u001b[0m\u001b[97m(-100, 9724)\u001b[0m \u001b[31m_\u001b[0m\u001b[97m(-100, 28730)\u001b[0m \u001b[31mcount\u001b[0m\u001b[97m(-100, 2114)\u001b[0m \u001b[31m',\u001b[0m\u001b[97m(-100, 647)\u001b[0m \u001b[31m '\u001b[0m\u001b[97m(-100, 464)\u001b[0m \u001b[31mErrors\u001b[0m\u001b[97m(-100, 16958)\u001b[0m \u001b[31m']\u001b[0m\u001b[97m(-100, 1421)\u001b[0m \u001b[31m\n",
      "\u001b[0m\u001b[97m(-100, 13)\u001b[0m \u001b[31m\n",
      "\u001b[0m\u001b[97m(-100, 13)\u001b[0m \u001b[31m###\u001b[0m\u001b[97m(-100, 27332)\u001b[0m \u001b[31m Response\u001b[0m\u001b[97m(-100, 12107)\u001b[0m \u001b[31m:\u001b[0m\u001b[97m(-100, 28747)\u001b[0m \u001b[31m\n",
      "\u001b[0m\u001b[97m(-100, 13)\u001b[0m \u001b[32m\n",
      "\u001b[0m\u001b[97m(13, 13)\u001b[0m \u001b[32m{\"\u001b[0m\u001b[97m(6799, 6799)\u001b[0m \u001b[32mbreak\u001b[0m\u001b[97m(2876, 2876)\u001b[0m \u001b[32mdown\u001b[0m\u001b[97m(3254, 3254)\u001b[0m \u001b[32ms\u001b[0m\u001b[97m(28713, 28713)\u001b[0m \u001b[32m\":\u001b[0m\u001b[97m(1264, 1264)\u001b[0m \u001b[32m [\"\u001b[0m\u001b[97m(7367, 7367)\u001b[0m \u001b[32mdb\u001b[0m\u001b[97m(2684, 2684)\u001b[0m \u001b[32m.\u001b[0m\u001b[97m(28723, 28723)\u001b[0m \u001b[32msystem\u001b[0m\u001b[97m(6574, 6574)\u001b[0m \u001b[32m\"],\u001b[0m\u001b[97m(8883, 8883)\u001b[0m \u001b[32m \"\u001b[0m\u001b[97m(345, 345)\u001b[0m \u001b[32mcal\u001b[0m\u001b[97m(1391, 1391)\u001b[0m \u001b[32mcul\u001b[0m\u001b[97m(2320, 2320)\u001b[0m \u001b[32mations\u001b[0m\u001b[97m(697, 697)\u001b[0m \u001b[32m\":\u001b[0m\u001b[97m(1264, 1264)\u001b[0m \u001b[32m [\u001b[0m\u001b[97m(733, 733)\u001b[0m \u001b[32m{\"\u001b[0m\u001b[97m(6799, 6799)\u001b[0m \u001b[32mcolumn\u001b[0m\u001b[97m(5571, 5571)\u001b[0m \u001b[32m\":\u001b[0m\u001b[97m(1264, 1264)\u001b[0m \u001b[32m \"\u001b[0m\u001b[97m(345, 345)\u001b[0m \u001b[32mduration\u001b[0m\u001b[97m(15458, 15458)\u001b[0m \u001b[32m_\u001b[0m\u001b[97m(28730, 28730)\u001b[0m \u001b[32mms\u001b[0m\u001b[97m(1033, 1033)\u001b[0m \u001b[32m\",\u001b[0m\u001b[97m(548, 548)\u001b[0m \u001b[32m \"\u001b[0m\u001b[97m(345, 345)\u001b[0m \u001b[32mop\u001b[0m\u001b[97m(410, 410)\u001b[0m \u001b[32m\":\u001b[0m\u001b[97m(1264, 1264)\u001b[0m \u001b[32m \"\u001b[0m\u001b[97m(345, 345)\u001b[0m \u001b[32mP\u001b[0m\u001b[97m(28753, 28753)\u001b[0m \u001b[32m9\u001b[0m\u001b[97m(28774, 28774)\u001b[0m \u001b[32m0\u001b[0m\u001b[97m(28734, 28734)\u001b[0m \u001b[32m\"}\u001b[0m\u001b[97m(17395, 17395)\u001b[0m \u001b[32m],\u001b[0m\u001b[97m(1181, 1181)\u001b[0m \u001b[32m \"\u001b[0m\u001b[97m(345, 345)\u001b[0m \u001b[32mfilters\u001b[0m\u001b[97m(16867, 16867)\u001b[0m \u001b[32m\":\u001b[0m\u001b[97m(1264, 1264)\u001b[0m \u001b[32m [\u001b[0m\u001b[97m(733, 733)\u001b[0m \u001b[32m{\"\u001b[0m\u001b[97m(6799, 6799)\u001b[0m \u001b[32mcolumn\u001b[0m\u001b[97m(5571, 5571)\u001b[0m \u001b[32m\":\u001b[0m\u001b[97m(1264, 1264)\u001b[0m \u001b[32m \"\u001b[0m\u001b[97m(345, 345)\u001b[0m \u001b[32mtrace\u001b[0m\u001b[97m(9213, 9213)\u001b[0m \u001b[32m.\u001b[0m\u001b[97m(28723, 28723)\u001b[0m \u001b[32mparent\u001b[0m\u001b[97m(3682, 3682)\u001b[0m \u001b[32m_\u001b[0m\u001b[97m(28730, 28730)\u001b[0m \u001b[32mid\u001b[0m\u001b[97m(313, 313)\u001b[0m \u001b[32m\",\u001b[0m\u001b[97m(548, 548)\u001b[0m \u001b[32m \"\u001b[0m\u001b[97m(345, 345)\u001b[0m \u001b[32mop\u001b[0m\u001b[97m(410, 410)\u001b[0m \u001b[32m\":\u001b[0m\u001b[97m(1264, 1264)\u001b[0m \u001b[32m \"\u001b[0m\u001b[97m(345, 345)\u001b[0m \u001b[32mdoes\u001b[0m\u001b[97m(23727, 23727)\u001b[0m \u001b[32m-\u001b[0m\u001b[97m(28733, 28733)\u001b[0m \u001b[32mnot\u001b[0m\u001b[97m(1478, 1478)\u001b[0m \u001b[32m-\u001b[0m\u001b[97m(28733, 28733)\u001b[0m \u001b[32mexist\u001b[0m\u001b[97m(22871, 22871)\u001b[0m \u001b[32m\"}\u001b[0m\u001b[97m(17395, 17395)\u001b[0m \u001b[32m],\u001b[0m\u001b[97m(1181, 1181)\u001b[0m \u001b[32m \"\u001b[0m\u001b[97m(345, 345)\u001b[0m \u001b[32morders\u001b[0m\u001b[97m(10486, 10486)\u001b[0m \u001b[32m\":\u001b[0m\u001b[97m(1264, 1264)\u001b[0m \u001b[32m [\u001b[0m\u001b[97m(733, 733)\u001b[0m \u001b[32m{\"\u001b[0m\u001b[97m(6799, 6799)\u001b[0m \u001b[32mcolumn\u001b[0m\u001b[97m(5571, 5571)\u001b[0m \u001b[32m\":\u001b[0m\u001b[97m(1264, 1264)\u001b[0m \u001b[32m \"\u001b[0m\u001b[97m(345, 345)\u001b[0m \u001b[32mduration\u001b[0m\u001b[97m(15458, 15458)\u001b[0m \u001b[32m_\u001b[0m\u001b[97m(28730, 28730)\u001b[0m \u001b[32mms\u001b[0m\u001b[97m(1033, 1033)\u001b[0m \u001b[32m\",\u001b[0m\u001b[97m(548, 548)\u001b[0m \u001b[32m \"\u001b[0m\u001b[97m(345, 345)\u001b[0m \u001b[32mop\u001b[0m\u001b[97m(410, 410)\u001b[0m \u001b[32m\":\u001b[0m\u001b[97m(1264, 1264)\u001b[0m \u001b[32m \"\u001b[0m\u001b[97m(345, 345)\u001b[0m \u001b[32mP\u001b[0m\u001b[97m(28753, 28753)\u001b[0m \u001b[32m9\u001b[0m\u001b[97m(28774, 28774)\u001b[0m \u001b[32m0\u001b[0m\u001b[97m(28734, 28734)\u001b[0m \u001b[32m\",\u001b[0m\u001b[97m(548, 548)\u001b[0m \u001b[32m \"\u001b[0m\u001b[97m(345, 345)\u001b[0m \u001b[32morder\u001b[0m\u001b[97m(2274, 2274)\u001b[0m \u001b[32m\":\u001b[0m\u001b[97m(1264, 1264)\u001b[0m \u001b[32m \"\u001b[0m\u001b[97m(345, 345)\u001b[0m \u001b[32mdesc\u001b[0m\u001b[97m(5916, 5916)\u001b[0m \u001b[32mending\u001b[0m\u001b[97m(2570, 2570)\u001b[0m \u001b[32m\"}\u001b[0m\u001b[97m(17395, 17395)\u001b[0m \u001b[32m],\u001b[0m\u001b[97m(1181, 1181)\u001b[0m \u001b[32m \"\u001b[0m\u001b[97m(345, 345)\u001b[0m \u001b[32mtime\u001b[0m\u001b[97m(1536, 1536)\u001b[0m \u001b[32m_\u001b[0m\u001b[97m(28730, 28730)\u001b[0m \u001b[32mrange\u001b[0m\u001b[97m(6347, 6347)\u001b[0m \u001b[32m\":\u001b[0m\u001b[97m(1264, 1264)\u001b[0m \u001b[32m \u001b[0m\u001b[97m(28705, 28705)\u001b[0m \u001b[32m7\u001b[0m\u001b[97m(28787, 28787)\u001b[0m \u001b[32m2\u001b[0m\u001b[97m(28750, 28750)\u001b[0m \u001b[32m0\u001b[0m\u001b[97m(28734, 28734)\u001b[0m \u001b[32m0\u001b[0m\u001b[97m(28734, 28734)\u001b[0m \u001b[32m}\u001b[0m\u001b[97m(28752, 28752)\u001b[0m \u001b[32m</s>\u001b[0m\u001b[97m(2, 2)\u001b[0m\u001b[39m\n",
      "[2025-01-23 16:22:10,536] [INFO] [axolotl.check_example_labels:44] [PID:2266767] [RANK:0] \n",
      "\n",
      "\n",
      "\u001b[39m\n",
      "[2025-01-23 16:22:10,536] [INFO] [axolotl.scripts.load_datasets:499] [PID:2266767] [RANK:0] printing prompters...\u001b[39m\n",
      "[2025-01-23 16:22:10,536] [INFO] [axolotl.scripts.load_datasets:501] [PID:2266767] [RANK:0] \n",
      "<start>\n",
      "\u001b[36mBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "{instruction}\n",
      "\n",
      "### Input:\n",
      "{input}\n",
      "\n",
      "### Response:\n",
      "{output}\u001b[39m\n",
      "<end>\n",
      "\u001b[39m\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  8.38it/s]\n",
      "[2025-01-23 16:22:11,185] [INFO] [axolotl.cli.preprocess.do_cli:75] [PID:2266767] [RANK:0] \u001b[32mSuccess! Preprocessed data path: `dataset_prepared_path: last_run_prepared`\u001b[39m\u001b[39m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m axolotl.cli.preprocess config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "with open('config.yaml', 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "model_id = cfg['base_model']\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "ds = load_from_disk('last_run_prepared/4ffa9e604bbfc10574f229b43f9234a2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Honeycomb is an observability platform that allows you to write queries to inspect trace data. You are an assistant that takes a natural language query (NLQ) and a list of valid columns and produce a Honeycomb query.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "NLQ: \"group by HTTP method\"\n",
      "\n",
      "Columns: ['query_string_num_tokens', 'query_string_length', 'data_queries', 'http.target', 'task.id', 'trace_root.http.target', 'topic', 'http.host', 'total_hits', 'db.user', 'domain_types', 'db.name', 'graphql.document', 'history', 'http.scheme', 'http.method', 'frontend.version', 'disposition_for_dBVVysC8x4Ymwg9rtjMckgw9', 'db.system', 'event_name', 'organization', 'auth.logout', 'organizations', 'name', 'net.transport', 'db.operation', 'disposition_for_UvsPPBVUn9FDuzDjsjYCqopq', 'disposition_for_1RUGSd7GdnP5tuKdgqBRZUm2', 'process.pid', 'disposition_for_6uyAoBc3PuvEcTTPFgPM3Rtk', 'exception.stacktrace', 'data_ingestion_individuals_count', 'disposition_for_qrnUBUz8YBfNX7Liekq6nKi3', 'task_type.type', 'disposition_for_JQDNbuUdaQcEbEwQNxUbV5EF', 'disposition_for_rAcWoXfbHw4eWoJFH4ZcY8ue', 'disposition_for_eShqQoC9jUi9VQBidpp2oXHP', 'parent_name', 'template', 'graphql.operation.name', 'span.num_links', 'disposition_for_kNSPtvsCWkDoEyFP2QE6VPmQ', 'disposition_for_UUqf9L1qkFxDNEvcgsVMA2yy', 'disposition_for_vwbbN76HZ7uitLubvkUjPFQE', 'disposition_for_aAto1pGrdF5RunpSX8sY5hvn', 'disposition_for_UbKCMdnkPQ6TuHrfdBo5juZu', 'disposition_for_QfrvmoHxSgLPJXPKZCrZfGo8', 'disposition_for_NoKSSruBRCX6UG28PzmkybUd', 'disposition_for_UZAqvZ5XVBZjKKWuMeRkRayS', 'organization_token', 'duration_ms', 'trace.parent_id', 'db.statement', 'exception.message', 'error', 'service.name', 'http.status_code', 'http.route']\n",
      "\n",
      "### Response:\n",
      "\n",
      "{\"breakdowns\": [\"http.method\"], \"calculations\": [{\"op\": \"COUNT\"}], \"time_range\": 7200}</s>\n"
     ]
    }
   ],
   "source": [
    "print(tok.decode(ds['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[2025-01-23 16:33:42,336] [INFO] [datasets.<module>:54] [PID:2272191] PyTorch version 2.5.1 available.\n",
      "[2025-01-23 16:33:42,844] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-01-23 16:33:42,877] [INFO] [root.spawn:60] [PID:2272191] gcc -pthread -B /home/aki/miniconda3/compiler_compat -fno-strict-overflow -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/aki/miniconda3/include -fPIC -O2 -isystem /home/aki/miniconda3/include -fPIC -c /tmp/tmp59k3jes5/test.c -o /tmp/tmp59k3jes5/test.o\n",
      "[2025-01-23 16:33:42,886] [INFO] [root.spawn:60] [PID:2272191] gcc -pthread -B /home/aki/miniconda3/compiler_compat /tmp/tmp59k3jes5/test.o -laio -o /tmp/tmp59k3jes5/a.out\n",
      "[2025-01-23 16:33:43,078] [INFO] [root.spawn:60] [PID:2272191] gcc -pthread -B /home/aki/miniconda3/compiler_compat -fno-strict-overflow -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/aki/miniconda3/include -fPIC -O2 -isystem /home/aki/miniconda3/include -fPIC -c /tmp/tmpco_wts98/test.c -o /tmp/tmpco_wts98/test.o\n",
      "[2025-01-23 16:33:43,086] [INFO] [root.spawn:60] [PID:2272191] gcc -pthread -B /home/aki/miniconda3/compiler_compat /tmp/tmpco_wts98/test.o -L/usr/local/cuda -L/usr/local/cuda/lib64 -lcufile -o /tmp/tmpco_wts98/a.out\n",
      "/home/aki/workspace/learning/finetuning/.venv/lib/python3.12/site-packages/axolotl/monkeypatch/relora.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n",
      "[2025-01-23 16:33:43,955] [INFO] [axolotl.normalize_config:211] [PID:2272191] [RANK:0] cuda memory usage baseline: 0.000GB (+1.533GB misc)\u001b[39m\n",
      "\n",
      "     #@@ #@@      @@# @@#\n",
      "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
      "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
      "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
      "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
      "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
      "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
      "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
      "    @@@@  @@@@@@@@@@@@@@@@\n",
      "\n",
      "[2025-01-23 16:33:45,447] [DEBUG] [axolotl.load_tokenizer:296] [PID:2272191] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
      "[2025-01-23 16:33:45,447] [DEBUG] [axolotl.load_tokenizer:297] [PID:2272191] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
      "[2025-01-23 16:33:45,447] [DEBUG] [axolotl.load_tokenizer:298] [PID:2272191] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
      "[2025-01-23 16:33:45,447] [DEBUG] [axolotl.load_tokenizer:299] [PID:2272191] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
      "[2025-01-23 16:33:45,447] [INFO] [axolotl.load_tokenized_prepared_datasets:216] [PID:2272191] [RANK:0] Unable to find prepared dataset in last_run_prepared/ba483ea2948b0421becad922b8b21686\u001b[39m\n",
      "[2025-01-23 16:33:45,447] [INFO] [axolotl.load_tokenized_prepared_datasets:217] [PID:2272191] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2025-01-23 16:33:45,447] [WARNING] [axolotl.load_tokenized_prepared_datasets:219] [PID:2272191] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset.\u001b[39m\n",
      "Generating train split: 100 examples [00:00, 48953.13 examples/s]\n",
      "[2025-01-23 16:33:46,299] [INFO] [axolotl.get_dataset_wrapper:613] [PID:2272191] [RANK:0] Loading dataset with base_type: alpaca and prompt_style: None\u001b[39m\n",
      "Tokenizing Prompts (num_proc=24): 100%|█| 100/100 [00:00<00:00, 603.95 examples/\n",
      "[2025-01-23 16:33:46,635] [INFO] [axolotl.load_tokenized_prepared_datasets:486] [PID:2272191] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/ba483ea2948b0421becad922b8b21686\u001b[39m\n",
      "Saving the dataset (1/1 shards): 100%|█| 100/100 [00:00<00:00, 29895.25 examples\n",
      "[2025-01-23 16:33:46,642] [DEBUG] [axolotl.calculate_total_num_steps:342] [PID:2272191] [RANK:0] total_num_tokens: 47_741\u001b[39m\n",
      "[2025-01-23 16:33:46,642] [DEBUG] [axolotl.calculate_total_num_steps:360] [PID:2272191] [RANK:0] `total_supervised_tokens: 6_994`\u001b[39m\n",
      "[2025-01-23 16:33:46,642] [DEBUG] [axolotl.calculate_total_num_steps:438] [PID:2272191] [RANK:0] total_num_steps: 6\u001b[39m\n",
      "[2025-01-23 16:33:46,644] [DEBUG] [axolotl.train.train:66] [PID:2272191] [RANK:0] loading tokenizer... mistralai/Mistral-7B-v0.1\u001b[39m\n",
      "[2025-01-23 16:33:47,348] [DEBUG] [axolotl.load_tokenizer:296] [PID:2272191] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
      "[2025-01-23 16:33:47,348] [DEBUG] [axolotl.load_tokenizer:297] [PID:2272191] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
      "[2025-01-23 16:33:47,348] [DEBUG] [axolotl.load_tokenizer:298] [PID:2272191] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
      "[2025-01-23 16:33:47,348] [DEBUG] [axolotl.load_tokenizer:299] [PID:2272191] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
      "[2025-01-23 16:33:47,348] [DEBUG] [axolotl.train.train:98] [PID:2272191] [RANK:0] loading model and peft_config...\u001b[39m\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.03s/it]\n",
      "[2025-01-23 16:33:49,878] [INFO] [axolotl.load_model:1088] [PID:2272191] [RANK:0] cuda memory usage after model load: 3.842GB (+0.013GB cache, +1.703GB misc)\u001b[39m\n",
      "[2025-01-23 16:33:49,885] [INFO] [axolotl.prepare_model:1008] [PID:2272191] [RANK:0] converting PEFT model w/ prepare_model_for_kbit_training\u001b[39m\n",
      "[2025-01-23 16:33:49,886] [INFO] [axolotl.load_model:1121] [PID:2272191] [RANK:0] Converting modules to torch.bfloat16\u001b[39m\n",
      "[2025-01-23 16:33:49,887] [INFO] [axolotl.load_lora:1310] [PID:2272191] [RANK:0] found linear modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[39m\n",
      "trainable params: 83,886,080 || all params: 7,325,618,176 || trainable%: 1.1451\n",
      "[2025-01-23 16:33:50,311] [INFO] [axolotl.load_model:1182] [PID:2272191] [RANK:0] cuda memory usage after adapters: 4.155GB (+0.836GB cache, +1.702GB misc)\u001b[39m\n",
      "/home/aki/workspace/learning/finetuning/.venv/lib/python3.12/site-packages/axolotl/core/trainer_builder.py:444: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AxolotlTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*_args, **kwargs)\n",
      "[2025-01-23 16:33:50,830] [INFO] [axolotl.train.train:141] [PID:2272191] [RANK:0] Pre-saving adapter config to ./qlora-alpaca-out\u001b[39m\n",
      "[2025-01-23 16:33:50,831] [INFO] [axolotl.train.train:178] [PID:2272191] [RANK:0] Starting trainer...\u001b[39m\n",
      "{'loss': 1.3005, 'grad_norm': 1.5576499700546265, 'learning_rate': 1e-05, 'epoch': 0.17}\n",
      " 20%|█████████                                    | 1/5 [00:24<01:37, 24.42s/it][2025-01-23 16:34:43,885] [INFO] [axolotl.callbacks.on_step_end:130] [PID:2272191] [RANK:0] cuda memory usage while training: 4.330GB (+3.241GB cache, +1.668GB misc)\u001b[39m\n",
      "{'loss': 1.2735, 'grad_norm': 1.4180048704147339, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      " 40%|██████████████████                           | 2/5 [00:52<01:20, 26.82s/it]\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|██████████████████████████████               | 2/3 [00:02<00:01,  1.05s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.209926962852478, 'eval_runtime': 5.261, 'eval_samples_per_second': 1.901, 'eval_steps_per_second': 0.57, 'epoch': 0.35}\n",
      " 40%|██████████████████                           | 2/5 [00:58<01:20, 26.82s/it]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:03<00:00,  1.07s/it]\u001b[A\n",
      "{'loss': 1.2286, 'grad_norm': 1.5508458614349365, 'learning_rate': 3e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2619, 'grad_norm': 1.4853547811508179, 'learning_rate': 4e-05, 'epoch': 0.7}\n",
      " 80%|████████████████████████████████████         | 4/5 [01:56<00:29, 29.83s/it]\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|██████████████████████████████               | 2/3 [00:02<00:01,  1.08s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.1222318410873413, 'eval_runtime': 5.37, 'eval_samples_per_second': 1.862, 'eval_steps_per_second': 0.559, 'epoch': 0.7}\n",
      " 80%|████████████████████████████████████         | 4/5 [02:01<00:29, 29.83s/it]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:03<00:00,  1.09s/it]\u001b[A\n",
      "{'loss': 1.2033, 'grad_norm': 1.5307585000991821, 'learning_rate': 5e-05, 'epoch': 0.87}\n",
      "{'train_runtime': 152.09, 'train_samples_per_second': 0.592, 'train_steps_per_second': 0.033, 'train_loss': 1.2535651922225952, 'epoch': 0.87}\n",
      "100%|█████████████████████████████████████████████| 5/5 [02:32<00:00, 30.42s/it]\n",
      "[2025-01-23 16:36:23,233] [INFO] [axolotl.train.train:195] [PID:2272191] [RANK:0] Training Completed!!! Saving pre-trained model to ./qlora-alpaca-out\u001b[39m\n",
      "adapter_model.safetensors:   0%|                     | 0.00/336M [00:00<?, ?B/s]\n",
      "Upload 2 LFS files:   0%|                                 | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "adapter_model.safetensors:   0%|          | 16.4k/336M [00:00<3:12:57, 29.0kB/s]\u001b[A\u001b[A\n",
      "\n",
      "training_args.bin: 100%|███████████████████| 6.65k/6.65k [00:01<00:00, 6.02kB/s]\u001b[A\u001b[A\n",
      "adapter_model.safetensors: 100%|█████████████| 336M/336M [00:44<00:00, 7.61MB/s]\n",
      "\n",
      "Upload 2 LFS files: 100%|█████████████████████████| 2/2 [00:44<00:00, 22.32s/it]\u001b[A\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!accelerate launch -m axolotl.cli.train config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge with base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-22 10:30:48,916] [INFO] [numexpr.utils._init_num_threads:149] [PID:1556838] Note: NumExpr detected 24 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "[2025-01-22 10:30:48,916] [INFO] [numexpr.utils._init_num_threads:162] [PID:1556838] NumExpr defaulting to 16 threads.\n",
      "[2025-01-22 10:30:49,006] [INFO] [datasets.<module>:54] [PID:1556838] PyTorch version 2.5.1 available.\n",
      "[2025-01-22 10:30:49,468] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-01-22 10:30:49,501] [INFO] [root.spawn:60] [PID:1556838] gcc -pthread -B /home/aki/miniconda3/compiler_compat -fno-strict-overflow -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/aki/miniconda3/include -fPIC -O2 -isystem /home/aki/miniconda3/include -fPIC -c /tmp/tmpkfddbhlc/test.c -o /tmp/tmpkfddbhlc/test.o\n",
      "[2025-01-22 10:30:49,539] [INFO] [root.spawn:60] [PID:1556838] gcc -pthread -B /home/aki/miniconda3/compiler_compat /tmp/tmpkfddbhlc/test.o -laio -o /tmp/tmpkfddbhlc/a.out\n",
      "[2025-01-22 10:30:49,808] [INFO] [root.spawn:60] [PID:1556838] gcc -pthread -B /home/aki/miniconda3/compiler_compat -fno-strict-overflow -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/aki/miniconda3/include -fPIC -O2 -isystem /home/aki/miniconda3/include -fPIC -c /tmp/tmpmui8o8zq/test.c -o /tmp/tmpmui8o8zq/test.o\n",
      "[2025-01-22 10:30:49,818] [INFO] [root.spawn:60] [PID:1556838] gcc -pthread -B /home/aki/miniconda3/compiler_compat /tmp/tmpmui8o8zq/test.o -L/usr/local/cuda -L/usr/local/cuda/lib64 -lcufile -o /tmp/tmpmui8o8zq/a.out\n",
      "/home/aki/workspace/learning/finetuning/.venv/lib/python3.12/site-packages/axolotl/monkeypatch/relora.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n",
      "\n",
      "     #@@ #@@      @@# @@#\n",
      "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
      "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
      "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
      "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
      "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
      "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
      "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
      "    @@@@  @@@@@@@@@@@@@@@@\n",
      "\n",
      "[2025-01-22 10:30:50,503] [INFO] [axolotl.normalize_config:211] [PID:1556838] [RANK:0] cuda memory usage baseline: 0.000GB (+0.838GB misc)\u001b[39m\n",
      "[2025-01-22 10:30:50,503] [INFO] [axolotl.common.cli.load_model_and_tokenizer:51] [PID:1556838] [RANK:0] loading tokenizer... mistralai/Mistral-7B-v0.1\u001b[39m\n",
      "[2025-01-22 10:30:51,261] [DEBUG] [axolotl.load_tokenizer:296] [PID:1556838] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
      "[2025-01-22 10:30:51,261] [DEBUG] [axolotl.load_tokenizer:297] [PID:1556838] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
      "[2025-01-22 10:30:51,261] [DEBUG] [axolotl.load_tokenizer:298] [PID:1556838] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
      "[2025-01-22 10:30:51,261] [DEBUG] [axolotl.load_tokenizer:299] [PID:1556838] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
      "[2025-01-22 10:30:51,261] [INFO] [axolotl.common.cli.load_model_and_tokenizer:53] [PID:1556838] [RANK:0] loading model and (optionally) peft_config...\u001b[39m\n",
      "[2025-01-22 10:30:51,818] [INFO] [accelerate.utils.modeling.get_balanced_memory:1014] [PID:1556838] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:06<00:00,  3.25s/it]\n",
      "[2025-01-22 10:30:58,634] [INFO] [axolotl.load_model:1088] [PID:1556838] [RANK:0] cuda memory usage after model load: 13.489GB (+0.126GB cache, +0.956GB misc)\u001b[39m\n",
      "[2025-01-22 10:30:58,643] [INFO] [axolotl.load_model:1121] [PID:1556838] [RANK:0] Converting modules to torch.bfloat16\u001b[39m\n",
      "[2025-01-22 10:30:58,645] [INFO] [axolotl.load_lora:1310] [PID:1556838] [RANK:0] found linear modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[39m\n",
      "[2025-01-22 10:30:58,645] [DEBUG] [axolotl.load_lora:1358] [PID:1556838] [RANK:0] Loading pretrained PEFT - LoRA\u001b[39m\n",
      "/home/aki/workspace/learning/finetuning/.venv/lib/python3.12/site-packages/lm_eval/filters/extraction.py:92: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n",
      "/home/aki/workspace/learning/finetuning/.venv/lib/python3.12/site-packages/lm_eval/filters/extraction.py:165: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  f\":[\\s]*({without_paren_fallback_regex})\"\n",
      "trainable params: 83,886,080 || all params: 7,325,618,176 || trainable%: 1.1451\n",
      "[2025-01-22 10:30:59,667] [INFO] [axolotl.load_model:1182] [PID:1556838] [RANK:0] cuda memory usage after adapters: 13.817GB (+1.072GB cache, +0.973GB misc)\u001b[39m\n",
      "[2025-01-22 10:30:59,979] [INFO] [axolotl.scripts.do_merge_lora:171] [PID:1556838] [RANK:0] running merge of LoRA with base model\u001b[39m\n",
      "Unloading and merging model: 100%|██████████| 647/647 [00:00<00:00, 7892.40it/s]\n",
      "[2025-01-22 10:31:00,064] [INFO] [axolotl.scripts.do_merge_lora:180] [PID:1556838] [RANK:0] saving merged model to: qlora-alpaca-out/merged\u001b[39m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m axolotl.cli.merge_lora config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.74s/it]\n"
     ]
    }
   ],
   "source": [
    "# model_id='hieu1344/hc-mistral-alpaca'\n",
    "model_path = 'qlora-alpaca-out/'\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(model_path, load_in_4bit=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(nlq, cols):\n",
    "    return f\"\"\"Honeycomb is an observability platform that allows you to write queries to inspect trace data. You are an assistant that takes a natural language query (NLQ) and a list of valid columns and produce a Honeycomb query.\n",
    "\n",
    "### Instruction:\n",
    "\n",
    "NLQ: \"{nlq}\"\n",
    "\n",
    "Columns: {cols}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "def prompt_tok(nlq, cols, return_ids=False):\n",
    "    _p = prompt(nlq, cols)\n",
    "    input_ids = tokenizer(_p, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "    out_ids = model.generate(input_ids=input_ids, max_new_tokens=5000, \n",
    "                          do_sample=False)\n",
    "    ids = out_ids.detach().cpu().numpy()\n",
    "    if return_ids: return out_ids\n",
    "    return tokenizer.batch_decode(ids, \n",
    "                                  skip_special_tokens=True)[0][len(_p):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlq = \"Exception count by exception and caller\"\n",
    "cols = ['error', 'exception.message', 'exception.type', 'exception.stacktrace']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "out = prompt_tok(nlq, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n{\"breakdowns\": [\"exception.message\", \"exception.type\"], \"calculations\": [{\"column\": \"exception.message\", \"op\": \"COUNT\"}], \"orders\": [{\"column\": \"exception.message\", \"op\": \"COUNT\", \"order\": \"descending\"}], \"time_range\": 7200}'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
